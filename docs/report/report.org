#+TITLE: A Weighted Least Squares Method for Time Delay Estimation in Gravitationally Lensed Photon Streams
#+AUTHOR: \Large{Micha{\l} Staniaszek} \\\small{Supervisor: Peter Ti≈ào}
#+EMAIL:     mxs968@cs.bham.ac.uk
#+DATE:      \today
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT:
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper,11pt]
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \usepackage[titletoc,page,title]{appendix}
#+LaTeX_HEADER: \usepackage{biblatex}
#+LaTeX_HEADER: \usepackage{metalogo}
#+LaTeX_HEADER: \bibliography{fyp}
#+LATEX_HEADER: \defaultfontfeatures{Mapping=tex-text}
#+LATEX_HEADER: \setromanfont[Ligatures={Common},Numbers={Lining}]{Linux Libertine}

\thispagestyle{empty}
\newpage
\pagenumbering{roman}
#+BEGIN_abstract
In this report, we present a system for the estimation of the
time delay $\Delta t$ between multiple realisations of a Poisson
process with the underlying function $\lambda(t)$, with particular
application to gravitationally lensed photon streams. We build on
the weighted least squares approach to develop a linear estimator 
which we use to estimate $\lambda(t)$. We then introduce a numerical
and probabilistic method for estimating $\Delta t$ using
the function estimates. Finally, we compare the performance of our
linear estimator to kernel regression on simulated data and data 
from real quasars.

\vspace{1.0cm}\textbf{Keywords: }Poisson process, gravitational lensing,
 machine learning, linear estimation

\begin{center}
\vspace*{\fill}\scriptsize{Typeset in Linux Libertine using \XeLaTeX}.
\end{center}
#+END_abstract
\newpage
#+begin_latex
\tableofcontents
\newpage
\pagenumbering{arabic}
#+end_latex
* Introduction
With continued advances in computing and sensing technologies, the amount of data that can be gathered from both everyday objects and scientific experiments has increased rapidly. However, more data is not always a blessing---it must be stored and analysed for it to have any use, and this is not an easy task when one has terabytes of data to deal with. The Large Hadron Collider at CERN is one perhaps extreme example, producing on the order of five terabytes of data each second. Storing this amount of data, let along analysing it is impossible, and so multiple stages of intelligent filtering are applied, reducing the throughput to 100 gigabytes per second, and then further to around 200 megabytes per second, where it is finally stored, producing almost two CDs each second \cite{WLCGproc}. This project focuses on creating the foundations for a system to do such intelligent filtering, but in the context of astronomical data. The volume of data produced by modern telescopes, while not on the same scale as the LHC, is nonetheless overwhelming. Image sizes of one to two gigabytes are not uncommon, and deciding what data is actually relevant is not a trivial task \cite{starck2002handbook}. Using intelligent filtering algorithms, it should be possible to flag up interesting-looking data for further study. While there are many areas in which such capabilities would be useful, we are particularly interested in finding candidates for images of gravitationally lensed objects. In order to do this, it is necessary to find pairs of observations of photon flux which appear to have the same underlying function. More precisely, given a set of data containing the time of arrival of photons from a particular source, henceforth called a \emph{stream}, we wish to find another stream which, when shifted in time by some value $\Delta t$, has similar numbers of photons arriving in a given interval as the first stream. We call $\Delta t$ the \emph{delay} between the two streams. In this project, we develop a system which can generate simulated photon streams using Poisson processes, use linear regression to estimate the underlying function of a given stream, and, given the function estimates of two streams, estimate the time delay between them. The concepts which underpin the project are discussed in more detail in the \hyperref[sec-2]{background} section.
** Problem statement
   we are trying to find the time delay between two or more photon
   streams which come from a gravitationally lensed object. In order
   to do this, we use a kernel regression method to estimate the
   underlying functions of each stream, and then use various
   techniques to determine whether the two streams of photons
   originate from the same object. After confirming that both streams
   are likely to come from the same object (with some confidence), we
   determine the time delay between the two streams.

form a base for a system which can automatically recognise potential
lensed objects and flag them for further investigation
** Impact of solving (or not solving) the problem
   Successful completion of the project will lead to a more effective
   technique for determining the time delay from gravitationally
   lensed photon streams. In particular, if the technique is more
   accurate than those currently used for the calculation of these
   delays, then it could allow more accurate estimation of things that
   are important to determine the structure of the universe. Time
   delay calculations have been used to determine the value of the
   hubble constant to a higher precision, as well as to find out the
   mass distribution in a certain area of space. Both of these are
   areas of great importance in astrophysics, and the ability to
   calculate these values more accurately could potentially lead to
   discoveries being made in other areas.
** how we will try and solve the problem
   In order to solve this problem, we will use estimation methods
   which allow the calculation of the rate parameter $\lambda$ at any
   time $t \in[0,T]$, where $T$ is the end of the time interval for
   which we have photon data. Once we are able to do this, we can take
   two streams of photons and compare the functions that we compute
   from the streams, and judge whether they are coming from the same
   lensed object or not, and at the same time calculate the time delay
   between the two streams. Whether the two streams have the same
   underlying function will be determined by using a maximum
   likelihood estimator. Intuitively, the point of maximum likelihood
   is the value of $\Delta t$ at which there is most overlap between
   the two functions. The two functions will be judged to be the same
   function when the return value of some error function drops below a
   specified threshold. Our estimate for the time delay $\Delta t$ is
   therefore the value that it takes at the point at which the error
   is lowest.
** steps to do that
   In order to build a system which can perform the required task, the
   following components will be necessary.
   
*** Photon simulator
    Although we have access to photon data for a specific
    gravitationally lensed object, it is useful for testing and
    experimentation purposes to have a simulator with which the
    efficacy of techniques can be evaluated. The most suitable way to
    simulate events in the natural world is by way of poisson
    processes. In the case of photon streams these will be
    non-homogeneous processes, since the rate at which photons arrive
    will be non-linear. We will create generators for both homogeneous
    and non-homogeneous processes, which will allow the specification
    of the function to be used to generate the photon stream. The
    generator will output data which we can use as the ground truth
    for experimentation.
*** Linear estimators
    While a linear estimator is not strictly necessary to complete the
    project, we feel that the implementation of these estimators will
    be a good method of increasing understanding of the process of
    estimation. The knowledge gained from writing these more basic
    estimators should mean that the theory behind the more complex
    estimator is approached with a deeper understanding of the
    foundations of estimation techniques, which should make its
    implementation easier to tackle, as well as resulting in an
    intuition as to good ways of proceeding. The first step of this
    stage is to implement a simple linear estimator, which will
    estimate functions of the form $y=a+bx$. We can then modify this
    to work as a piecewise estimator for non-linear functions similar
    in form to those that the photon data is likely to take.
*** Error function for time delay estimation
    When the maximum likelihood estimator is completed, we should be
    able to estimate non-linear functions to a high degree of
    accuracy, which is necessary to improve the estimation of the time
    delay. The more accurate our estimate of the function that
    underlies the photon stream, the better we will be able to
    estimate the delay. To perform this estimation, we will need an
    error function which we can use to determine the error between the
    two photon streams. With this error value, we can give a
    confidence value to each value of $\Delta t$, and choose the one
    which produces the lowest error for the two streams.
* Background
** Time Delay Calculation
- Talk generally about the problem of time delay estimation
- refer to physics papers attempting to make estimates of the delay
- talk about time delay estimation in particular, refer to kundic et al, many others
- talk about how better estimates benefit the scientific community
- refer to peter's paper about the efficacy of kernel regression
- better estimators are necessary to increase the accuracy of estimates
- this is an experiment to see whether this method has any use
- build on technique introduced in massey et al
** Gravitational Lensing
   - what is it
   - how does it occur
   - pictures
   - what does it tell us about the universe or things in it
** Poisson Processes
   - where do they occur
   - what can they be used for
   - Times for a homogeneous poisson process can be generated using
     the formula $\log\frac{-U}{\lambda}$
** Linear Regression
* Simulation of Photon Streams
** Function Generation
** Generating Streams from Functions
* Function Estimation
** Baseline Estimation
*** Optimum Least Squares
*** Iterative Weighted Least Squares
*** Piecewise Iterative Weighted Least Squares
    Initially, we thought that it may be possible to decide whether to
    extend the line or not based on the difference in slope between
    the estimate from the previous time interval and the estimate of
    the next. If the previous estimate was positive, and the next
    negative, and vice versa, clearly the line should not be
    continued. The intercept parameter was considered to be much less
    important. However, this assumption was highly flawed. Due to the
    nature of poisson processes, it is perfectly possible that
    although the function has changed significantly after the end of
    the previous interval, the estimate for the interval that we are
    trying to extend the line into could return very similar values to
    that of the previous interval. Because of this, we extend the line
    when we should not be doing so. There are several potential
    solutions to this problem. First, rather than forming a new
    estimate, we extend the line and then check how much the error has
    increased. If it goes over a certain threshold, then we reject the
    extension attempt and try again, this time with a shorter
    extension. Another potential way of improving the piecewise
    estimation in general would be to require the estimate for the
    next time period to start from the end point of the last time
    period. This would prevent the intercept parameter from changing,
    and would force the estimator to find the best estimate given a
    specific starting point, rather than giving it free reign to find
    the estimate which actually minimises the error.
**** coding issues
what to do with the issue of minimum length of intervals? Sometimes
not extending the original gives a better estimate of the line than
re-estimating the interval extended, or adding the short interval onto
the end of the previous one and using its estimate. See data in the
min_interval_length folder in data. The better fitting line is the
baseline estimate of that with no minimum, and the other set is the
estimate with minimum interval length applied
*** Baseline
** Kernel Density Estimation
* Time Delay Calculation
** Area Method
** Probability Mass Function Method
* System
** System Structure
*** Overall Structure
*** Estimators
*** Generators
*** Experimenter
** Development
*** Development Process
*** Version Control
    - branching strategy
    - commit frequency
    - using issues on github
*** Project Management
    - keep changelog
    - writing up and planning layout in notebook
* Evaluation
** experimentation on simulated data
** experimentation on real-world data
* Conclusion
\newpage
\nocite{*}
\printbibliography
\newpage
#+BEGIN_appendices
* Installation
** MuParser
 download package\\
 run \texttt{./configure --prefix=/usr}, followed by \texttt{make \&\& make install} (may require sudo)
 this installs muparser so that headers can be found in \texttt{/usr/include}
    
    sudo apt-get install libgsl0-dev check 

#+END_appendices
