#+TITLE: A Weighted Least Squares Method for Time Delay Estimation in Gravitationally Lensed Photon Streams
#+AUTHOR: \Large{Micha{\l} Staniaszek} \\\small{Supervisor: Peter Ti≈ào}
#+EMAIL:     mxs968@cs.bham.ac.uk
#+DATE:      \today
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT:
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper,11pt]
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \usepackage[titletoc,page,title]{appendix}
#+LaTeX_HEADER: \usepackage{biblatex}
#+LaTeX_HEADER: \usepackage{metalogo}
#+LaTeX_HEADER: \usepackage{graphicx}
#+LaTeX_HEADER: \bibliography{fyp}
#+LATEX_HEADER: \defaultfontfeatures{Mapping=tex-text}
#+LATEX_HEADER: \setromanfont[Ligatures={Common},Numbers={Lining}]{Linux Libertine}

\thispagestyle{empty}
\newpage
\pagenumbering{roman}
#+BEGIN_abstract
In this report, we present a system for the estimation of the time delay $\Delta
t$ between multiple realisations of a Poisson process with the underlying
function $\lambda(t)$, with particular application to gravitationally lensed
photon streams. We build on the weighted least squares approach to develop a
linear estimator which we use to estimate $\lambda(t)$. We then introduce a
numerical and probabilistic method for estimating $\Delta t$ using the function
estimates. Finally, we compare the performance of our linear estimator to kernel
regression on simulated data and data from real quasars.

\vspace{1.0cm}\textbf{Keywords: }Poisson process, gravitational lensing,
 machine learning, linear estimation

\begin{center}
\vspace*{\fill}\scriptsize{Typeset in Linux Libertine using \XeLaTeX}.
\end{center}
#+END_abstract
\newpage
#+begin_latex
\tableofcontents
\newpage
\pagenumbering{arabic}
#+end_latex
* Introduction
With continued advances in computing and sensing technologies, the amount of
data that can be gathered from both everyday objects and scientific experiments
has increased rapidly. However, more data is not always a blessing---it must be
stored and analysed for it to have any use, and this is not an easy task when
one has terabytes of data to deal with. The Large Hadron Collider at CERN is one
perhaps extreme example, producing on the order of five terabytes of data each
second. Storing this amount of data, let along analysing it is impossible, and
so multiple stages of intelligent filtering are applied, reducing the throughput
to 100 gigabytes per second, and then further to around 200 megabytes per
second, where it is finally stored, producing almost two CDs each second
\cite{WLCGproc}. This project focuses on creating the foundations for a system
to do such intelligent filtering, but in the context of astronomical data. The
volume of data produced by modern telescopes, while not on the same scale as the
LHC, is nonetheless overwhelming. Image sizes of one to two gigabytes are not
uncommon, and deciding what data is actually relevant is not a trivial task
\cite{starck2002handbook}. Using intelligent filtering algorithms, it should be
possible to flag up interesting-looking data for further study. While there are
many areas in which such capabilities would be useful, we are particularly
interested in finding candidates for images of gravitationally lensed
objects. In order to do this, it is necessary to find pairs of observations of
photon flux which appear to have the same underlying function. More precisely,
given a set of data containing the time of arrival of photons from a particular
source, henceforth called a \emph{stream}, we wish to find another stream which,
when shifted in time by some value $\Delta$, has similar numbers of photons
arriving in a given interval as the first stream. We call $\Delta$ the
\emph{delay} between the two streams. In this project, we develop a system which
can generate simulated photon streams using Poisson processes, use linear
regression to estimate the underlying function of a given stream, and, given the
function estimates of two streams, estimate the time delay between them. Knowing
the value of the time delay has many applications in astrophysics, and with more
precise estimates, more accurate calculations can be made to increase our
understanding of the universe we live in.

In section [[Background]] we discuss the concepts underpinning the project in more
detail, with a more in-depth explanation of the issues surrounding the
calculation of the time delay and its uses. In section [[Simulation of Photon
Streams]] we introduce our method of generating photon streams from Poisson
processes. Section [[Function Estimation]] shows our approach to estimating the
underlying function of a given stream of photons. Our methods of calculating the
time delays between multiple photon streams are explained in section [[Time Delay
Calculation]]. Section [[System]] gives detailed information on the design and
development of the system, including the software and project management
aspects. Finally, in section [[Evaluation]] we present experimental data from both
simulated and real data and discuss the relative effectiveness of our methods.
* Background
** Gravitational Lensing
In an eight-year period starting in 1907 and ending in 1915 with the publication
of a paper on field equations of gravitation\cite{einstein1915general}, Albert
Einstein wrote many papers developing a new theory of gravitation, his general
theory of relativity. This generalisation of special relativity and Newton's law
of universal gravitation led to a revolution in the field of physics, and
remains one of the most important scientific discoveries to date. The theory
describes how spacetime is affected by the presence of matter and radiation, and
this idea has many important consequences, but one of the effects in particular
is important in the context of this report.

According to the theory, objects with mass, or massive objects, cause spacetime
to curve around them. A simple way to visualise this effect is to imagine
dropping a ball onto a sheet of cloth which has been pulled taut. The ball will
eventually come to a stop in the centre of the cloth, and cause it to sag. Here,
the sheet represents spacetime, and the ball represents anything from planets,
to stars, or even entire galaxies. Depending on the weight of the ball, the
shape of the cloth will be affected to different degrees---a ping pong ball will
have hardly any effect at all, but if we drop a bowling ball onto the sheet, the
effect will be significant. In a similar way, the amount that spacetime curves
around a massive object depends on its mass. An object with high mass will cause
a large amount of curvature, whereas a lower mass object will cause less. If a
second ball, lighter than the first, is introduced to the system, what happens?
With no initial velocity, it will roll in a straight line towards the first ball
sitting at the centre of the sheet. This is one way of thinking about gravity
and its relationship with spacetime---an object's gravitational attraction is a
result of its mass curving spacetime, and the strength of the attraction is
proportional to the mass. While objects with no mass, such as photons, cannot be
affected by gravity directly, they \emph{are} affected by the curvature of
spacetime. This bending of light rays is known as
\emph{gravitational lensing}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{einstein_ring}
\caption{An example of a gravitational lensing effect known as an Einstein ring.
In this image, the central galaxy, LRG 3-757, is distorting light from a distant
blue galaxy \cite{einsteinring}.}
\label{fig:einsteinring}
\end{figure}
- what is it
- how does it occur
- pictures
- what does it tell us about the universe or things in it
*** Importance of the Time Delay
- Talk generally about the problem of time delay estimation
- refer to physics papers attempting to make estimates of the delay
- talk about time delay estimation in particular, refer to kundic et al, many others
- talk about how better estimates benefit the scientific community
- refer to peter's paper about the efficacy of kernel regression
- better estimators are necessary to increase the accuracy of estimates
- this is an experiment to see whether this method has any use
- build on technique introduced in massey et al
** Poisson Processes
   - where do they occur
   - what can they be used for
   - Times for a homogeneous poisson process can be generated using
     the formula $\log\frac{-U}{\lambda}$
** Linear Regression
* Simulation of Photon Streams
** Function Generation
** Generating Streams from Functions
* Function Estimation
** Baseline Estimation
*** Optimum Least Squares
*** Iterative Weighted Least Squares
*** Piecewise Iterative Weighted Least Squares
    Initially, we thought that it may be possible to decide whether to
    extend the line or not based on the difference in slope between
    the estimate from the previous time interval and the estimate of
    the next. If the previous estimate was positive, and the next
    negative, and vice versa, clearly the line should not be
    continued. The intercept parameter was considered to be much less
    important. However, this assumption was highly flawed. Due to the
    nature of poisson processes, it is perfectly possible that
    although the function has changed significantly after the end of
    the previous interval, the estimate for the interval that we are
    trying to extend the line into could return very similar values to
    that of the previous interval. Because of this, we extend the line
    when we should not be doing so. There are several potential
    solutions to this problem. First, rather than forming a new
    estimate, we extend the line and then check how much the error has
    increased. If it goes over a certain threshold, then we reject the
    extension attempt and try again, this time with a shorter
    extension. Another potential way of improving the piecewise
    estimation in general would be to require the estimate for the
    next time period to start from the end point of the last time
    period. This would prevent the intercept parameter from changing,
    and would force the estimator to find the best estimate given a
    specific starting point, rather than giving it free reign to find
    the estimate which actually minimises the error.
**** coding issues
what to do with the issue of minimum length of intervals? Sometimes
not extending the original gives a better estimate of the line than
re-estimating the interval extended, or adding the short interval onto
the end of the previous one and using its estimate. See data in the
min_interval_length folder in data. The better fitting line is the
baseline estimate of that with no minimum, and the other set is the
estimate with minimum interval length applied
*** Baseline
** Kernel Density Estimation
* Time Delay Calculation
** Area Method
** Probability Mass Function Method
* System
** System Structure
*** Overall Structure
*** Estimators
*** Generators
*** Experimenter
** Development
*** Development Process
*** Version Control
    - branching strategy
    - commit frequency
    - using issues on github
*** Project Management
    - keep changelog
    - writing up and planning layout in notebook
* Evaluation
** experimentation on simulated data
** experimentation on real-world data
* Conclusion
\newpage
\nocite{*}
\printbibliography
\newpage
#+BEGIN_appendices
* Installation
** MuParser
 download package\\
 run \texttt{./configure --prefix=/usr}, followed by \texttt{make \&\& make install} (may require sudo)
 this installs muparser so that headers can be found in \texttt{/usr/include}
    
    sudo apt-get install libgsl0-dev check 

#+END_appendices
