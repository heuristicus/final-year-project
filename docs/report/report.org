#+TITLE: Time Delay Estimation in Gravitationally Lensed Photon Stream Pairs
#+AUTHOR: \Large{Micha{\l} Staniaszek} \\\small{Supervisor: Peter Ti≈ào}
#+EMAIL:     mxs968@cs.bham.ac.uk
#+DATE:      \today
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT:
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper,11pt]
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \usepackage[titletoc,page,title]{appendix}
#+LaTeX_HEADER: \usepackage{biblatex}
#+LaTeX_HEADER: \usepackage{metalogo}
#+LaTeX_HEADER: \usepackage{graphicx}
#+LaTeX_HEADER: \usepackage{moreverb}
#+LaTeX_HEADER: \usepackage[center]{caption}
#+LaTeX_HEADER: \usepackage{subcaption}
#+LaTeX_HEADER: \let\iint\relax % otherwise errors are thrown by amsmath. Defined in latexsym
#+LaTeX_HEADER: \let\iiint\relax
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{hyperref}
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usetikzlibrary{positioning}
#+LaTeX_HEADER: \bibliography{fyp}
#+LATEX_HEADER: \defaultfontfeatures{Mapping=tex-text}
#+LATEX_HEADER: \setromanfont[Ligatures={Common},Numbers={Lining}]{Linux Libertine}

\thispagestyle{empty}
\newpage
\pagenumbering{roman}
#+BEGIN_abstract
In this report, we present a system for estimating the time delay $\Delta$
between multiple realisations of a Poisson process with the underlying function
$\lambda(t)$, with particular application to gravitationally lensed photon
streams. We develop a linear estimator based on weighted least squares, and a
kernel density estimator which we use to estimate $\lambda(t)$. We then
introduce two methods for estimating the value of $\Delta$ using the function
estimates, one using inter-function area, and another using probability density
functions. Finally, we compare the performance of the two function estimation
methods and time delay estimation methods on simulated data, and show that there
is not a significant difference between the approaches.

\vspace{1.0cm}\textbf{Keywords: }Poisson process, gravitational lensing,
 machine learning, linear estimation

\begin{center}
\vspace*{\fill}\scriptsize{Typeset in Linux Libertine using \XeLaTeX}.
\end{center}
#+END_abstract
\newpage
#+begin_latex
\tableofcontents
\newpage
\pagenumbering{arabic}
#+end_latex
* Introduction
With continued advances in computing and sensing technologies, the amount of
data that can be gathered from both everyday objects and scientific experiments
has increased rapidly. However, more data is not always a blessing---it must be
stored and analysed for it to have any use, and this is not an easy task when
one has terabytes of data to deal with. The Large Hadron Collider at CERN is one
perhaps extreme example, producing on the order of five terabytes of data each
second. Storing this amount of data, let along analysing it is impossible, and
so multiple stages of intelligent filtering are applied, reducing the throughput
to 100 gigabytes per second, and then further to around 200 megabytes per
second, where it is finally stored, producing almost two CDs each second
\cite{WLCGproc}. This project focuses on creating the foundations for a system
to do such intelligent filtering, but in the context of astronomical data. The
volume of data produced by modern telescopes, while not on the same scale as the
LHC, is nonetheless overwhelming. Image sizes of one to two gigabytes are not
uncommon, and deciding what data is actually relevant is not a trivial task
\cite{starck2002handbook}. Using intelligent filtering algorithms, it should be
possible to flag up interesting-looking data for further study. While there are
many areas in which such capabilities would be useful, we are particularly
interested in finding candidates for images of gravitationally lensed
objects. In order to do this, it is necessary to find pairs of observations of
photon flux which appear to have the same underlying function. More precisely,
given a set of data containing the time of arrival of photons from a particular
source, henceforth called a \emph{stream}, we wish to find another stream which,
when shifted in time by some value $\Delta$, has similar numbers of photons
arriving in a given interval as the first stream. We call $\Delta$ the
\emph{delay} between the two streams. In this project, we develop a system which
can generate simulated photon streams using Poisson processes, use linear
regression to estimate the underlying function of a given stream, and, given the
function estimates of two streams, estimate the time delay between them. Knowing
the value of the time delay has many applications in astrophysics, and with more
precise estimates, more accurate calculations can be made to increase our
understanding of the universe we live in.

- strong lensing has delays on the order of hundreds of days, but weak lensing
  is more like on the order of hours - no longer sufficient to calculate flux
  for a single day, must do it in a different way, by measuring individual
  photon arrival times.

In section [[Background]] we discuss the concepts underpinning the project in more
detail, with a more in-depth explanation of the issues surrounding the
calculation of the time delay and its uses. In section [[Simulation of Photon
Streams]] we introduce our method of generating photon streams from Poisson
processes. Section [[Function Estimation]] shows our approach to estimating the
underlying function of a given stream of photons. Our methods of calculating the
time delays between multiple photon streams are explained in section [[Time
Delay Estimation]]. Section [[System]] gives detailed information on the design and
development of the system, including the software and project management
aspects. Finally, in section [[Evaluation]] we present experimental data from both
simulated and real data and discuss the relative effectiveness of our methods.
* Background
** Gravitational Lensing
In an eight-year period starting in 1907 and ending in 1915 with the publication
of a paper on field equations of gravitation \cite{einstein1915general}, Albert
Einstein wrote many papers developing a new theory of gravitation, his general
theory of relativity. This generalisation of special relativity and Newton's law
of universal gravitation led to a revolution in the field of physics, and
remains one of the most important scientific discoveries to date. The theory
describes how spacetime is affected by the presence of matter and radiation, and
this idea has many important consequences, but one of the effects in particular
is important in the context of this report.

According to the theory, objects with mass, or massive objects, cause spacetime
to curve around them. A simple way to visualise this effect is to imagine
dropping a ball onto a sheet of cloth which has been pulled taut. The ball will
eventually come to a stop in the centre of the cloth, and cause it to sag. Here,
the sheet represents spacetime, and the ball represents anything from planets,
to stars, or even entire galaxies. Depending on the weight of the ball, the
shape of the cloth will be affected to different degrees---a ping pong ball will
have hardly any effect at all, but if we drop a bowling ball onto the sheet, the
effect will be significant. In a similar way, the amount that spacetime curves
around a massive object depends on its mass. An object with high mass will cause
a large amount of curvature, whereas a lower mass object will cause less. If a
second ball, lighter than the first, is introduced to the system, what happens?
With no initial velocity, it will roll in a straight line towards the first ball
sitting at the centre of the sheet. This is one way of thinking about gravity
and its relationship with spacetime---an object's gravitational attraction is a
result of its mass curving spacetime, and the strength of the attraction is
proportional to the mass. While objects with no mass, such as photons, cannot be
affected by gravity directly, they \emph{are} affected by the curvature of
spacetime. This bending of light rays is known as
\emph{gravitational lensing}.

The first person to study the effects of gravitational lensing was Orest
Chvolson, publishing a short note to \emph{Astronomische Nachrichten} in 1924
\cite{chwolsonlensing}. However, the concept was largely unknown until a short
calculation by Einstein was published in \emph{Science} in 1936
\cite{einsteinlensing}. Interestingly, Chvolson's note appears directly above a
note from Einstein\cite{einsteinchwolson}, but there appears to be no evidence that Einstein had ever
seen it \cite{renn2000eclipses}. The first gravitationally
lensed object to be identified was the twin quasar SBS 0957+561, in 1979, and
since then, over a hundred such objects have been discovered
\cite{firstlens,gravlenscount}. The effect of gravitational lensing is, as the
name suggests, similar to that of a lens, such as that of a camera. Unlike a
camera lens, however, gravitational lenses do not have a focal point, but
instead a focal line, resulting in images such as that shown in Figure
\ref{fig:einring} if the source (the object being lensed), the lensing object
(the massive object around which the light is being bent) and the observer lie on a
straight line. This effect is relatively rare, however, and in general rather
than a ring, multiple images of the source can be observed. In these so called
\emph{strong} lensing effects, the distortion is very clearly visible. However,
two other classes of lensing exist---\emph{weak lensing} and
\emph{microlensing}.  The effects of weak lensing cannot easily be observed
visually, but statistical techniques can show the distortion
produced. Microlensing works on even smaller scales than the other two classes,
and can be used to detect planets and stars. It has also been proposed as a
method to find objects such as black holes and brown dwarfs, which are otherwise
difficult to detect \cite{schneider2006gravitational}.
\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=\textwidth]{einstein_ring}
\caption{An Einstein ring}
\label{fig:einring}
\end{subfigure}
\qquad
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=\textwidth]{einstein_cross}
\caption{Einstein's cross}
\label{fig:einsteincross}
\end{subfigure}
\caption{Two examples of strong lensing effects. a) shows light from
a distant blue galaxy being distorted by the central galaxy LRG 3-757
\cite{einsteinring}. b) shows four images of a distant quasar being lensed by a
foreground galaxy \cite{eincross}.}
\label{fig:stronglens}
\end{figure}
*** Importance of the Time Delay
In gravitationally lensed systems, there is a delay between photon streams
coming from images of the source due to the bending of light. Light from one
source may have had to travel a slightly longer distance than that from the
other, and while photons travel extremely fast, over astronomical distances the
delay can become quite large. 
- strong lensing p86
- Talk generally about the problem of time delay estimation
- refer to physics papers attempting to make estimates of the delay
- talk about time delay estimation in particular, refer to kundic et al, many others
- talk about how better estimates benefit the scientific community
- refer to peter's paper about the efficacy of kernel regression
- better estimators are necessary to increase the accuracy of estimates
- this is an experiment to see whether this method has any use
- build on technique introduced in massey et al
** Poisson Processes
In certain situations, there are many benefits of having good models of the
numbers of events that occur in a given period. For example, being able to
estimate the number of incoming requests to a server, the number of calls made
to emergency services, and the rate of radioactive decay at any given time are
all useful in different applications. Poisson processes are \emph{stochastic
processes} that can be used to do just that. A stochastic process is a way of
representing the evolution of a random value or system over time by using
collections of random variables. Most such processes do not evolve in a
\emph{deterministic} way. That is, the way they change as time passes is not
predictable.

A Poisson process is one such process which counts the number of events and the
time at which they occur in a given time interval, and have been used to model
all of the above examples
\cite{hajjam2008approach,cannizzaro1978results,arlitt1997internet}. In their
basic form, Poisson processes have the following important properties
\cite{ross1997simulation}:
1. $N(0)=0$.
   - $N(t)$ represents the total number of events that occurred up until time
     $t$. Thus, if $N(0)=0$, it follows that the process begins at $t=0$.
2. The numbers of events occurring in disjoint time intervals are independent.
   - The \emph{independent increment} assumption. This states that $N(t)$, the
     number of events that occur up to time $t$ is \emph{independent} of the
     number $N(t+s)-N(t)$, i.e. the number of events in the time interval
     between $t$ and $s$. In other words, the number of events that occur in one
     interval does not have an effect on the number of events in any other time
     interval.
3. The probability distribution of the number of events that occur in a given
   interval is dependent only on the length of the interval.
   - The \emph{stationary increment} assumption. The implication of this is that
     the probability distribution of $N(t+s)-N(t)$ is the same for all values of
     $t$. That is, the likelihood of a number of events $n$ occurring in the
     above time interval does not change, regardless of the value of $t$.
4. No counted occurrences are simultaneous.
   - For all events that occur in the duration of the process, no two events
     will occur at the same time.

The most important thing about Poisson processes is the \emph{rate parameter},
$\lambda$. This value represents the number of events that occur in each time
interval. As we are counting events, it is clear that the rate parameter can
never go below zero---there cannot be a negative number of occurrences in a
given time interval. There are two types of Poisson processes,
\emph{homogeneous} and \emph{non-homogeneous}. In a homogeneous Poisson process (HPP),
the rate parameter is constant throughout the running of the process. This means
that in every interval, the same number of events are likely to occur. In
contrast, a non-homogeneous Poisson process (NHPP) has a rate parameter which
varies. This means that the rate at which events occur varies during the running
time of the process.
** Function Estimation
*** Linear Regression
Linear regression is a statistical technique used to fit lines or curves to data
points in order to find some sort of relationship between them. The number of
variables in the data is important. One of the variables is called a \emph{dependent}
variable. We want to find the relationship between this variable and the other
variables, called \emph{independent} variables. What makes one variable
dependent and another independent? Consider the expression $y=f(x)$. If $f(x)$
is some function of the variable $x$, then we know that the value of $y$ depends
on the value of $x$. This is where the names come from. In this simple example,
$x$ is the independent variable, and $y$ is the dependent variable. There can be
multiple independent variables.

Linear regression is used in many different fields to find the trend between
variables. It is heavily used in economics to make predictions about what
happens in many economical situations. Finding trends in data is useful to many
people in different ways.
*** Kernel Density Estimation
This is another method which can be used to estimate functions, but which
applies specifically to the probability density function of random
variables. This technique uses \emph{kernels} to estimate the function
densities. A kernel is a function which has some parameters. To estimate
functions, kernels are centred at certain points along the axis which is being
estimated. The spread can be either at uniform intervals, each sample value,
etc. Kernels may have a weight assigned to them. Varying the parameters of
the kernels results in different properties of the estimate. There are many
different kernels that can be used. Different kernels are used in different
applications.
- Show some examples of different kernels
* Simulation of Photon Streams
The first step in building the system was the development of a photon stream
simulator. The ability to simulate photon streams means that the system can be
tested on many different stream types, so that we are able to determine where
its strengths and weaknesses lie. While many simulation tools are very complex,
our system does not require simulation of the source objects or the movement of
photons, as we are only interested in their arrival time. A source can be
represented by some random variable $X$, which indicates the variability of the
source with time. Different types of sources will have different types of
characteristic functions---the variation in a quasar will be very different to
that of an individual star, for example. A NHPP is an ideal way to represent
this type of system. The function $\lambda(t)$ will represent the random
variable, and the values output from the process will represent the arrival
times of the photons. $\lambda(t)$ describes the variability of the source in
time. In other words, it provides a rate parameter at each time $t$ for the
duration of the simulation. To be able to simulate a wide variety of functions,
it is necessary to have the capacity to generate functions with different
characteristics.
** Function Generation
To evaluate the performance of the function and time delay estimators, it will
be necessary to test the accuracy of the estimates on different types of
functions. To this end, the capability of generating random functions will be
very useful. To generate random functions, we make use of \large{Gaussian
kernels or just Gaussians?}. The generation process involves four simple steps:
1. Pick some value $\Delta t$ which represents the distance between the mean
   $\mu$ of successive Gaussians.
2. Define some value $\alpha$, where the standard deviation $\sigma$ of each
   Gaussian is determined by $\alpha\cdot\Delta t$.
3. For each Gaussian, choose some weight $w_i$, from a uniform distribution
   between -1 and 1.
4. Using some step $s$, sum all the Gaussians at each point on the x-axis which
   we get from these $s$ values.

The first step defines how spread out the Gaussians should be in the interval
$[t_0, T]$ in which the function is to be generated. If the spread is large,
then depending on the standard deviation of the Gaussians there will be many
points on the interval where the value of the function is zero. On the other
hand, with a low value of $\Delta t$, most points on the line should have some
non-zero value.

The $\alpha$ parameter determines the standard deviation $\sigma$ of all the Gaussians
used to generate the function. The value of $\sigma$ is the one that affects the
final function the most. Low values will result in each Gaussian covering only a
small interval, so if the Gaussians are sufficiently spread out, the variation
in the function will be much larger than if higher values of $\sigma$ are used.

With just the above two steps, the functions generated would be very
homogeneous, because each Gaussian has the same weight. With uniform Gaussians,
there would be hills at each point where a Gaussian is centred, and very little
to speak of in between, and the height of the function would never exceed a
certain value. To introduce more variation, a weight $w_i$ is sampled uniformly
from ($w_i \sim U(-1,1)$). Uniform sampling simply means that each value between
-1 and 1 has an equal probability of being chosen. To further increase the
variation in the functions that can be generated, some multiplier can be used,
which scales the values of the weights, meaning that the function will have
larger values over the whole interval.

The final step is to calculate the values which will make up the
function. Starting at the beginning of the interval $t_0$, we sum the values of
all the Gaussians at points along the line until the end of the interval $T$ is
reached. The points that are sampled are defined by $t_i=t_{i-1}+s$, where $s$
is some sample step. The use of smaller sample steps results in a higher
resolution. The sum of the Gaussians at time $t$ can be calculated by

\begin{align}
f(t) = \sum_{g\in G}w_g\cdot e^{-(t-\mu_g)^2/2\sigma_g^2}
\end{align}

Where $G$ is the set of Gaussians which make up the function, and $w_g$, $\mu_g$
and $\sigma_g$ are the weight, mean and standard deviation respectively of the
current Gaussian being processed.

- random functions using gaussian sums
- explain how the gaussians are placed, how the standard deviation is
   calculated, what effect this has on the function shape
   ($\sigma=\alpha\cdot\Delta t$)
- examples at various alpha and delta t values?
- $w_t\cdot e^\frac{{x-\mu}^2}{2\sigma^2}$

In addition to the random function generation, it may sometimes be useful to
generate a function from a known expression, and the system includes this
functionality as well, which will be described below.
** Generating Streams from Functions
   Once the function has been generated, we can use it to generate values for
   the random variable $X$ which governs a NHPP. To generate a NHPP, it is
   necessary to build on the generation of a HPP. It is well known that
   probability of an event occurring follows an exponential distribution. The
   rate parameter $\lambda$ determines how many events occur in a given time
   interval. Knowing this, we can calculate the time of the next event by
   sampling from this distribution. Generate a random value $U\sim U(0,1)$. The
   time $t$ to the next event is defined by 
   \begin{align}\label{eq:homlambda}
   t=\frac{1}{\lambda}\log(U)
   \end{align}
   Using this calculation, it is possible to generate a realisation of a HPP for
   any length of time. This provides a base which can be extended to generate
   events from NHPPs. To generate events from the NHPP, we use a technique
   called thinning. The basic concept behind thinning is to generate a large
   amount of values, and then remove them based on some method. In the case of
   the NHPP, we generate events with a rate parameter $\lambda$, where
   $\lambda>\lambda(t)$ for $0<=t<=T$. In other words, the homogeneous lambda
   value must be larger than the value of the function we are generating from at
   any point. First, two random values are independently sampled from a uniform
   distribution between 0 and 1, $U_1,U_2\sim U(0,1)$. $U_1$ is used in
   \eqref{eq:homlambda} to find the next event time from the homogeneous process
   governed by $\lambda$. Using the time $t$ generated from that, the value of
   $\lambda(t)$ is found. Depending on the ratio between $\lambda(t)$ and
   $\lambda$, the event is kept or discarded. When the value of $\lambda(t)$ is
   close to that of $\lambda$, more events are kept because
   $U_2\leq\frac{\lambda(t)}{\lambda}$ will be true more of the time. The
   variation of $\lambda(t)$ in time means that events are generated
   proportional to the value of lambda.

- Need to generate event times - use Poisson process
- start with homogeneous
- extend homogeneous to non-homogeneous (explain math)
- Issues with the implementation - must have $\lambda>\lambda(t)$ for all
  $0<=t<=T$.
- Diagram showing HPP and NHPP
- http://preshing.com/20111007/how-to-generate-random-timings-for-a-poisson-process
** Implementation
    The implementation of the random and expression based function generators
    form the first part of the \emph{generator} sub-system. This part of the
    system deals with the generation of functions, and the generation of photon
    streams from these functions.
- gaussians as structs
- gauss vector structs
- functions into math library
- separation of functions so that they can be called externally and internally
- what can be generated
- How expr and gauss version differ
*** Generating from Expressions
- muparser
- enter expression and define variables in parameter file
- parsed in and calculated automatically
*** Generating from Random Functions
- 
* Function Estimation
** Baseline Estimation
*** Ordinary Least Squares
- Explain main parameters, the main things that are important to the workings,
  and exactly how it works. Number of intervals, the total time, see massey paper
- math
*** Iterative Weighted Least Squares
- Extension of the OLS technique which iterates it multiple times, updating weights
- show weight update equation
- talk about how varying the number of iterations affects it (massey)
- Show it estimating a line
- Explain how line estimation is not useful since lensed photon streams are
  always some function which is not a line.
*** Piecewise Iterative Weighted Least Squares
- explain intuition behind the technique. Split the whole interval into some
  finite number of subintervals and estimate the function of each interval in
  turn using IWLS.
- give reasoning behind moving to this technique. Some parts of functions look
  like they are pretty much linear - maybe it is a nice way to solve
  them. mention that this was developed on my own interest in seeing how it worked
- Explain the not-so-good parts - each subsection estimate is disjoint from the
  next, but the stream must be a continuous function.
- Talk about line extension and the minimum length issue

    Initially, we thought that it may be possible to decide whether to
    extend the line or not based on the difference in slope between
    the estimate from the previous time interval and the estimate of
    the next. If the previous estimate was positive, and the next
    negative, and vice versa, clearly the line should not be
    continued. The intercept parameter was considered to be much less
    important. However, this assumption was highly flawed. Due to the
    nature of poisson processes, it is perfectly possible that
    although the function has changed significantly after the end of
    the previous interval, the estimate for the interval that we are
    trying to extend the line into could return very similar values to
    that of the previous interval. Because of this, we extend the line
    when we should not be doing so. There are several potential
    solutions to this problem. First, rather than forming a new
    estimate, we extend the line and then check how much the error has
    increased. If it goes over a certain threshold, then we reject the
    extension attempt and try again, this time with a shorter
    extension. Another potential way of improving the piecewise
    estimation in general would be to require the estimate for the
    next time period to start from the end point of the last time
    period. This would prevent the intercept parameter from changing,
    and would force the estimator to find the best estimate given a
    specific starting point, rather than giving it free reign to find
    the estimate which actually minimises the error.
**** coding issues
what to do with the issue of minimum length of intervals? Sometimes
not extending the original gives a better estimate of the line than
re-estimating the interval extended, or adding the short interval onto
the end of the previous one and using its estimate. See data in the
min_interval_length folder in data. The better fitting line is the
baseline estimate of that with no minimum, and the other set is the
estimate with minimum interval length applied
*** Baseline
- improvement on the piecewise method by making sure that the function is
  continuous, i.e. the start of the function at each interval is the end of the
  one in the previous
- How we calculate the points at which to join the functions - do it at the
  midpoint on y between the start and end
- show a baseline and piecewise estimate on the same function
** Kernel Density Estimation
- explain how kernels are calculated, and how they are centred
- explain the effect of the standard deviation on the estimate
- needs normalisation to get the proper estimate
- how the normalisation constant is found, using pmf stuff
* Time Delay Estimation
- basic explanation of what we want to do with this part of the system,
  referring to the introduction a little?
** Area Method
\begin{figure}
%   \begin{center}
%     \begin{tikzpicture}[scale=0.1]
%       \def\fzero{est_out_func_0_0.dat}
%       \def\fone{est_out_func_0_1.dat}
%       \draw[thick] plot[smooth] file {\fzero};
%       \draw[thick] plot[smooth] file {\fone};
%       \fill[color = gray!60, opacity=0.1] plot[smooth] file {\fzero} --
%       plot[smooth] file {\fone};
% %      \fill[color = gray!60, opacity=0.1] plot[smooth] file {\fzero} --
% %      plot[smooth] file {\fone};
      
% %      \fill[pattern color = gray, pattern = north east lines, opacity=0.8]
%  %     plot[smooth] file {\fzero} --  plot[smooth] file {\fzero};
%       % \begin{axis}[stack plots=y,thick,smooth,no markers]
% %         \addplot+[black]              gnuplot{est_out_func_0_0.dat};
% %         \addplot+[black,fill=blue!50] gnuplot{est_out_func_0_1.dat} 
% %         \closedcycle;
% %       \end{axis}
% % %      \begin{axis}[xmin=0, xmax=100, ymin=0, ymax=25, legend columns=4]
% %         % \addplot[mark=none,color=red] table[x index=0,y index=1,col
% %         % sep=space] {est_out_func_0_0.dat};
% %         % \addplot[mark=none, color=blue] table[x index=0,y index=1,col
% %         % sep=space] {est_out_func_0_1.dat};
% %         % \addplot[mark=none, color=green, opacity=0.5] table[x index=0,y index=1,col
% %         % sep=space] {rfuncsum_2.dat};
% % %      \end{axis}
%     \end{tikzpicture}
%   \end{center}
\end{figure}
- use the area of the space between two functions, find the time delay which
  minimises the value
- show integral formula and then show the simplified discrete formula
** Probability Mass Function Method
- calculate probability mass function at each point on the function and choose
  the time delay which maximises the value
* System
** System Structure
   \begin{figure}
   \centering
   \pgfdeclarelayer{background}
   \pgfdeclarelayer{foreground}
   \pgfsetlayers{background,main,foreground}
   % horizontal separation
   \def \hnsep {0.5}
   \tikzstyle{sub}=[draw, fill=blue!20, text width=5em, 
   text centered, minimum height=2.5em, node distance=1.5cm]

   \begin{tikzpicture}
   \node (param) at (0,3.5) [sub] {Parameter file};
   % libs group
   \node (math) at (2,6) [sub] {Math};
   \node (gut) [sub, right=\hnsep of math] {General};
   \node (file) [sub, right=\hnsep of gut] {File};
   \node (plist) [sub, right=\hnsep of file] {Parameter List};
   \node (lib) [below right=0.25cm and -0.65 of gut] {\textbf{Libraries}};
   % generator group
   \node (hom) at (2,1) [sub] {HPP};
   \node (nhm) [sub, below of=hom] {NHPP};
   \node (rfunc) [sub, below of=nhm] {Random Function};
   \node (gauss) [sub, below of=rfunc] {Gaussian};
   \node (gen) [below of=gauss, font=\small] {\textbf{Generators}};
   \node (strout) [sub, below of=gen] {Stream Data};
   % estimator group
   \node (ln) at (6,0) [sub] {Linear};
   \node (pc) [sub, below of=ln] {Piecewise};
   \node (bl) [sub, below of=pc] {Baseline};
   \node (kd) [sub, below of=bl] {Kernel Density};
   \node (td) [sub, below of=kd] {Time Delta};
   \node (est) [below of=td, font=\small] {\textbf{Estimators}};
   \node (estout) [sub, below of=est] {Estimator Output};
   % experimenter
   \node (expparam) at (11.5,2) [sub] {Experiment Parameters};
   \node (exp) at (10,0) [sub] {Harness};
   \node (explbl) [below of=exp, font=\small] {\textbf{Experimenter}};
   \node (expout) [sub, below of=explbl] {Experiment Results};
   % Draw the rest on the background layer
   \begin{pgfonlayer}{background}
   % Estimator background
   \path (ln.north west)+(-0.2,0.2) node (a) {};
   \path (est.south -| ln.east)+(+0.2,-0.2) node (b) {};
   \path[fill=blue!10,rounded corners, draw=black!50, dashed]
   (a) rectangle (b);
   % generator background
   \path (hom.north west)+(-0.2,0.2) node (c) {};
   \path (gen.south -| hom.east)+(+0.2,-0.2) node (d) {};
   \path[fill=blue!10,rounded corners, draw=black!50, dashed]
   (c) rectangle (d);
   % libs background
   \path (math.north west)+(-0.2,0.2) node (e) {};
   \path (lib.south -| plist.east)+(+0.2,-0.2) node (f) {};
   \path[fill=blue!10,rounded corners, draw=black!50, dashed]
   (e) rectangle (f);
   % experimenter background
   \path (exp.north west)+(-0.2,0.2) node (g) {};
   \path (explbl.south -| exp.east)+(+0.2,-0.2) node (h) {};
   \path[fill=blue!10,rounded corners, draw=black!50, dashed]
   (g) rectangle (h);
   
   % path from expparam to experiments
   \coordinate [above=1.51 of exp] (expln) {};
   \coordinate [above=1 of exp] (tpexp) {};
   \draw [dashed,line width=1pt] (expparam.west) -- (expln);
   % path from experiments to exp out
   \draw [->,line width=1pt] (explbl.south)+(0,-0.2) -- (expout.north);

   % library arrows
   \path (ln.north)+(0,0.05) node (esttop){};    
   \coordinate [above=0.2cm of hom] (gentop) {};
   \coordinate [below=2cm of lib] (lsplit) {};
   \coordinate [below=0.2cm of lib] (blwlib) {};
   \coordinate [above=0.2cm of exp] (abvexp) {};
   \draw [-,line width=1pt] (blwlib) -- (lsplit);
   \draw [->,line width=1pt] (lsplit) -- (esttop);
   \draw [->,line width=1pt] (lsplit) -| (abvexp);
   \draw [->,line width=1pt] (lsplit) -| (gentop);

   % path from param to library link
   \coordinate [above=0.8cm of lsplit] (tt) {};
   \draw [dashed,line width=1pt] (param.east) -- (tt);
   
   % estimator arrows
   \draw [->] (ln.south) -- (pc.north);
   \draw [->] (pc.south) -- (bl.north);
   \coordinate [below=0.2 of est] (blest) {};
   \draw [->,line width=1pt] (blest)--(estout);
   \coordinate [right=0.9 of estout] (restout) {};
   \draw [dashed,line width=1pt] (estout.east) -- (restout);
   \draw [dashed,line width=1pt] (restout) |- (tpexp);
   
   % generator arrows
   \coordinate [above= 1 of ln] (abvln) {}; %above length est
   \coordinate [below=0.2 of gen] (bgen) {};
   \coordinate [right=0.9 of strout] (rstrout) {};
   \draw [->,line width=1pt] (bgen) -- (strout);
   \draw [->] (hom.south) -- (nhm.north);
   \draw [dashed,line width=1pt] (strout.east) -- (rstrout);
   \draw [dashed,line width=1pt] (rstrout) |- (abvln);
   
   \end{pgfonlayer}
   %\node (lib) at (0,0) [sub] {Libraries};
   %\node (est) at (1.5,-1) [sub] {Estimators};
   %\node (gen) at (-1.5,-1) [sub] {Generators};
   %\draw [->] (lib.east) -| (est.north);
   %\draw [->] (lib.west) -| (gen.north);
   \end{tikzpicture}
   \caption{System structure}
   \label{fig:sysstruct}
   \end{figure}
*** Overall Structure
*** Libraries
    - say what the function of each library is and what sort of functions it
      contains.
*** Estimators
Maybe this stuff should be in each individual section rather than here? talk
about how the system is interlinked in those sections - makes more sense with
the flow of things?
**** Function estimators
- talk about each estimator and where it is used, and how they are related.
- OLS->IWLS->Piecewise->Baseline
- gaussian + normalisation
**** Time delay estimators
*** Generators
- muparser generator
*** Interface
** Development
*** Development Process
- first draft up code in notebook to get down the concept
- write a basic code skeleton and add tests to make sure that it works as
  intended - particularly for mathematics and the like
- flesh out the code and integrate it with the system
- make code as modular as possible to make it easy to add stuff in later
*** Development methodologies
- could be merged with previous section, but mention some of the unix rules of
  thumb, like the rule of least surprise and so on.
*** Testing
- talk about check, which functions were tested (mostly library functions, hard
  to test estimators in a reliable way)
- give examples of the tests
*** Version Control
    - branching strategy
    - commit frequency
    - using issues on github
    - storing backups of tags on svn
*** Project Management
    - keep changelog
    - show examples of changelogs and commit messages from the same time period
    - writing up and planning layout in notebook
* Evaluation
** experimentation on simulated data
Talk about why there is a window on the effective values.
*** Sine Functions
   - $y=a-b\sin(\alpha t)$
   - five values of $\alpha$: 0.05, 0.1, 0.15, 0.3, 0.6
   - 50 pairs of realisations for each value of alpha
   - first find optimal parameter setting to estimate the functions by using
     model selection
   - Take the parameters from that and run the time delay estimator on the
     functions to see what results they give for the time delay.
   - Compare the means and standard deviations etc of the gaussian and baseline
     estimators to find out how good they are by using t-tests.
*** Random Functions
   - Streams in reality are not likely to follow sine curves, so additional
     experiments on randomly generated functions were also performed.
** experimentation on real-world data
* Conclusion
** Future Work and Improvements
- Poisson generator - must set lambda to be greater than the value of the
  function at all points, otherwise it breaks. There are some methods of fixing this
- baseline estimator - better way of joining functions - use the pmf to
  calculate the best possible point at which to join all points on the
  estimates. Use a hierarchical method to scan across the points between the
  points on the y axis and find the best values for all breakpoints
\newpage
\printbibliography
\newpage
#+BEGIN_appendices
* Installation
** MuParser
 download package\\
 run \texttt{./configure --prefix=/usr}, followed by \texttt{make \&\& make install} (may require sudo)
 this installs muparser so that headers can be found in \texttt{/usr/include}
 \begin{verbatimtab}   
 sudo apt-get install libgsl0-dev check 
 \end{verbatimtab}
* Usage
** Creating functions for experimentation
   Generate 10 random functions using gaussians, and output the transforms as
   well so they can be plotted
   \begin{verbatimtab}
   ./launcher -g ../data/params.txt -r -t 2 -c 10
   \end{verbatimtab}
   Generate two streams from each of these generated functions using the
   gaussian generator
   \begin{verbatimtab}
   ./launcher -g ../data/params.txt -f rand -n 2 -c 10
   \end{verbatimtab}
   Generate stuttered streams from files in this directory, so that you can
   perform model fitting to find the best parameters to use on the generated
   set.
   \begin{verbatimtab}
   ./launcher -x ../data/exp_params.txt -p ../data/params.txt -c 10 -n 2 -s -i .
   \end{verbatimtab}
   
#+END_appendices
